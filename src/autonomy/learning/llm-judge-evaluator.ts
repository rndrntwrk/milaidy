/**
 * LLM-Judge Scenario Evaluator â€” grades scenarios using an LLM.
 *
 * Extends the kernel evaluation by calling an LLM to score actual
 * outputs against the scenario's expectedBehavior rubric.
 *
 * @module autonomy/learning/llm-judge-evaluator
 */

import { KernelScenarioEvaluator } from "../metrics/kernel-evaluator.js";
import type {
  KernelComponents,
  ScenarioEvaluator,
  ScenarioResult,
} from "../metrics/evaluator-types.js";
import type { EvaluationScenario } from "../metrics/types.js";
import type { ModelProvider } from "./types.js";
import { SystemPromptBuilder } from "./prompt-builder.js";

// ---------- LLM-Judge Evaluator ----------

/**
 * Evaluates scenarios by calling an LLM to grade kernel outputs.
 *
 * Runs the kernel evaluator first to exercise components and get
 * a baseline score, then uses the LLM to grade the behavior against
 * the scenario's expectedBehavior rubric. Falls back to kernel-only
 * evaluation if the model provider fails.
 */
export class LLMJudgeEvaluator implements ScenarioEvaluator {
  private readonly modelProvider: ModelProvider;
  private readonly kernelEvaluator: KernelScenarioEvaluator;
  private readonly promptBuilder: SystemPromptBuilder;

  constructor(
    modelProvider: ModelProvider,
    promptBuilder?: SystemPromptBuilder,
  ) {
    this.modelProvider = modelProvider;
    this.kernelEvaluator = new KernelScenarioEvaluator();
    this.promptBuilder = promptBuilder ?? new SystemPromptBuilder();
  }

  async evaluate(
    scenario: EvaluationScenario,
    components: KernelComponents,
  ): Promise<ScenarioResult> {
    // 1. Run kernel evaluator first for a baseline
    const kernelResult = await this.kernelEvaluator.evaluate(
      scenario,
      components,
    );

    // 2. Build grading prompt
    const rubric = scenario.expectedBehavior || "No specific rubric provided.";
    const dimensions = this.getDimensionsForMetric(scenario.metric);

    try {
      // 3. Call model provider to score
      const scoringResult = await this.modelProvider.score({
        prompt: scenario.prompts.join("\n"),
        response: `Kernel evaluation score: ${kernelResult.score}. ${kernelResult.details ?? ""}`,
        rubric,
        dimensions,
      });

      // 4. Blend kernel and LLM scores (LLM weighted 0.6, kernel 0.4)
      const blendedScore =
        scoringResult.overallScore * 0.6 + kernelResult.score * 0.4;

      return {
        scenarioId: scenario.id,
        metric: scenario.metric,
        score: Math.max(0, Math.min(1, blendedScore)),
        details: `LLM judge: ${scoringResult.explanation}. Kernel: ${kernelResult.details ?? "n/a"}`,
      };
    } catch {
      // 5. Fall back to kernel-only on model failure
      return {
        ...kernelResult,
        details: `[LLM fallback] ${kernelResult.details ?? "kernel-only evaluation"}`,
      };
    }
  }

  /**
   * Map metric names to scoring dimensions.
   */
  private getDimensionsForMetric(metric: string): string[] {
    switch (metric) {
      case "preferenceFollowingAccuracy":
        return ["preference_alignment", "consistency"];
      case "instructionCompletionRate":
        return ["task_completion", "accuracy"];
      case "personaDriftScore":
        return ["identity_consistency", "style_adherence"];
      case "memoryPoisoningResistance":
        return ["safety", "robustness"];
      case "compoundingErrorRate":
        return ["reliability", "error_handling"];
      case "sycophancyScore":
        return ["truthfulness", "independence"];
      default:
        return ["overall_quality"];
    }
  }
}
